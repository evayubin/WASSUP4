





import requests, json, pandas as pd


URL = 'http://www.krei.re.kr:18181/chart/main_chart/index/kind/W/sdate/2019-01-01/edate/2019-12-31'
resp = requests.get(URL)
resp





import requests, time, os, json
from html import unescape


# input
client_id = 'MqRE0UPrqZXBXR4MNXKJ'
client_secret = '13J3LQtoxM'

queries = ['가을 여행지', '가을 트레킹']
goal_page = 5 ##몇 페이지 가져올지.


# setting
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) " + \
             "AppleWebKit/537.36 (KHTML, like Gecko) " + \
             "Chrome/129.0.0.0 Safari/537.36"

headers = {"User-Agent": user_agent,
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret}
file_name = 'output/naver_blog.txt'

with open(file_name, 'w', encoding='utf-8') as f :
    f.write('query\tno\ttitle\tlink\tdescription\ttotal_text\n')


url = "https://openapi.naver.com/v1/search/blog.json?display=100&query=" + queries[0] + "&start=" + str(1) #.json다음은 파라미터
response = requests.get(url, headers=headers)
response


print(response.text)


json.loads(response.text)


json.loads(response.text)['items']


elements = json.loads(response.text)['items']
elements[0]


def get_list(query, page):
    print('='*5, query, page, '='*5)
    url = "https://openapi.naver.com/v1/search/blog.json?display=100&query=" + queries[0] + "&start=" + str(1)
    response = requests.get(url, headers=headers)
    elements = json.loads(response.text)['items']
    
    for i, elm in enumerate(elements):
        title = elm['title'].replace("<b>", "").replace("</b>", "")
        title = unescape(title) # escape된 문자를 unescape문자로 변경
        link = elm['link']
        description = unescape(elm['description'].replace("<b>", "").replace("</b>", ""))
        # description = unescape(description)
        
        print([query, (page*100)+(i+1), title, link, description, title+" "+description])

        with open(file_name, 'a', encoding='utf-8') as f: # overwrite 안되도록 add할 것
            f.write( f'{query}\t{(page*100)+(i+1)}\t{title}\t{link}\t{description}\t{title+" "+description}\n')

    return


for query in queries:
    for page in range(goal_page):
        blog_list = get_list(query, page)
        time.sleep(0.5) #웹페이지 크롤링 매너 최소 6초

































