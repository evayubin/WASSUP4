


import requests
from bs4 import BeautifulSoup


PATH = "https://finance.naver.com/"
resp = requests.get(PATH)


resp


src = resp.text


soup = BeautifulSoup(src, 'lxml')
print(soup)


# quiz : 주요뉴스의 목록을 추출해보자
# 뉴스 목록 찾기
srclist = soup.select('.section_strategy a')
srclist


url = srclist[0]['href']
PATH + url


from urllib.parse import urljoin
urljoin(PATH, url)


# quiz : 네이버 증권 주요뉴스의 제목과 url 을 모두 추출하기
# 제목 변수, url 변수 만들기
# 제목과 url 데이터 목록 만들기
news_title = []
news_url = []

for item in srclist:
    title = item.text
    url = urljoin(PATH, item['href'])
    print(title, url)
    news_title.append(title)
    news_url.append(url)   


# 데이터프레임 변환
import pandas as pd
df = pd.DataFrame( {'제목':news_title, '주소':news_url}   )
df


pwd


df.to_excel





df.to_excel('output/naver_finance.xlsx', index=False)


import time


today = time.localtime()
today


today.tm_wday


tim


df.to_excel(f'output/{today.tm_year}_{today.tm_mon}naver_finance.xlsx', index=False)


# yyyy-mm-dd
'%d-%02d-%02d'%(today.tm_year, today.tm_mon, today.tm_mday)


file_date = '%d-%02d-%02d'%(today.tm_year, today.tm_mon, today.tm_mday)
excel_name = file_date + '.xlsx'
excel_name


file_date = '%d-%02d-%02d'%(today.tm_year, today.tm_mon, today.tm_mday)
csv_name = file_date + '.csv'
csv_name


df.to_csv(f'output/{csv_name}')


# 멜론차트, 노래제목과 가수 수집하기
# https://www.melon.com/chart/index.htm

# import requests
# from bs4 import BeautifulSoup
# import pandas as pd
# PATH = 'https://www.melon.com/chart/index.htm'
# resp = requests.get(PATH)
# resp
# <Response [406]>
# resp.text
# ''
# info = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}
# info


import requests
from bs4 import BeautifulSoup
import pandas as pd


PATH = 'https://www.melon.com/chart/index.htm'
resp = requests.get(PATH)
resp


# 406 error는 클라이언트 문제다. 너 누구야! 하는 것. 그럼 증명해주면 됨.


info = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'}
info


resp = requests.get(PATH, headers = info)
resp


html_src = resp.text
html_src


# dom 객체 변환
soup = BeautifulSoup(html_src, 'lxml')
soup


soup.select('.ellipsis.rank01 a')


len(soup.select('.ellipsis.rank01 a'))


title_list =[]
title = soup.select('.ellipsis.rank01 a')
for i in title:
    title_list.append(i.text)
title_list


# 리스트 컴프리핸션 > 훨씬 더 빠른 방법ㅇㅁㅇ
title= [i.text for i in soup.select('.ellipsis.rank01 a')]
title


song = [item.text for item in soup.select('.ellipsis.rank01 a')]
song


artist = [item.text for item in soup.select('.ellipsis.rank02 a')]
artist


len(soup.select('.ellipsis.rank02 a'))


artist = [item.text for item in soup.select('.ellipsis.rank02>a')]
artist


artist = [item.text for item in soup.select('.checkEllipsis')]
artist


len(soup.select('.checkEllipsis'))


# 순위 인덱스 만들기
rank = list(range(1,101))
print(rank)


songDF = pd.DataFrame({'순위':rank,
                      '노래제목': song,
                      '가수명': artist})

songDF


songDF.to_csv('output/melon100.csv')








param = {'name':'Amy', 'age':20, 'address':'seoul'}


#get
resp1 = requests.get('https://httpbin.org/get', params=param)
print(resp1)


print(resp1.text)


resp2 = requests.post('https://httpbin.org/post', data=param)
print(resp2)


print(resp2.text)








PATH = 'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5'
PATH


keyword = input('serch:')


resp = requests.get(PATH, params = {'query':keyword})
resp


html_src = resp.text


# 돔객체 만들기
soup = BeautifulSoup(html_src, 'lxml')


title = [i.text for i in soup.select('.news_contents a')]
title


result = soup.select('.news_tit')
print(len(result))
print(result)


srclist = soup.select('.news_contents a')
srclist


url = urljoin(PATH, item['href'])
print(url)


for a in result:
    print(a.text, a['href'])





PATH = "https://korean.visitkorea.or.kr/search/search_list.do?keyword=&area=All"
PATH


resp = requests.get(PATH)
resp


html_src = resp.text
html_src


soup = BeautifulSoup(html_src, 'lxml')
soup
























